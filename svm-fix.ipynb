{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "548bbb2d",
   "metadata": {},
   "source": [
    "# Sistem Klasifikasi Emosi menggunakan SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc3bc222",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LBP Extractor\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "class LBPExtractor:\n",
    "    \"\"\"Local Binary Pattern feature extractor\"\"\"\n",
    "    \n",
    "    def __init__(self, radius=1, n_points=8):\n",
    "        self.radius = radius\n",
    "        self.n_points = n_points\n",
    "    \n",
    "    def extract_lbp(self, image):\n",
    "        \"\"\"Extract LBP features from grayscale image\"\"\"\n",
    "        if len(image.shape) == 3:\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        h, w = image.shape\n",
    "        lbp_image = np.zeros_like(image)\n",
    "        \n",
    "        # Generate sampling points\n",
    "        angles = 2 * np.pi * np.arange(self.n_points) / self.n_points\n",
    "        sample_points = np.array([\n",
    "            [int(round(self.radius * np.cos(angle))),\n",
    "             int(round(self.radius * np.sin(angle)))]\n",
    "            for angle in angles\n",
    "        ])\n",
    "        \n",
    "        for i in range(self.radius, h - self.radius):\n",
    "            for j in range(self.radius, w - self.radius):\n",
    "                center_pixel = image[i, j]\n",
    "                lbp_code = 0\n",
    "                \n",
    "                for k, (dx, dy) in enumerate(sample_points):\n",
    "                    neighbor_pixel = image[i + dy, j + dx]\n",
    "                    if neighbor_pixel >= center_pixel:\n",
    "                        lbp_code |= (1 << k)\n",
    "                \n",
    "                lbp_image[i, j] = lbp_code\n",
    "        \n",
    "        # Calculate histogram\n",
    "        hist, _ = np.histogram(lbp_image.ravel(), bins=2**self.n_points, range=(0, 2**self.n_points))\n",
    "        hist = hist.astype(float)\n",
    "        hist /= (hist.sum() + 1e-7)  # Normalize\n",
    "        \n",
    "        return hist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d9ae859",
   "metadata": {},
   "source": [
    "## LBP Feature\n",
    "\n",
    "Fungsi dari LBP pada proses ini, yaitu untuk menangkap **tekstur lokal dalam wajah**.\n",
    "\n",
    "### Rumus:\n",
    "Untuk setiap piksel pusat `Ic`, bandingkan dengan tetangga `Ip`:\n",
    "\n",
    "```\n",
    "LBP = Σ_{p=0}^{P-1} s(Ip - Ic) * 2^p\n",
    "```\n",
    "\n",
    "Dengan fungsi `s(x)` sebagai:\n",
    "\n",
    "```\n",
    "s(x) = {\n",
    "    1, jika x >= 0\n",
    "    0, jika x < 0\n",
    "}\n",
    "```\n",
    "\n",
    "- `P` = Total Neighbour\n",
    "- `Ic` = Center Pixel Intensity\n",
    "- `Ip` = intensity of the p-th neighbor pixel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74b4af49",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HOGExtractor:\n",
    "    \"\"\"Histogram of Oriented Gradients feature extractor\"\"\"\n",
    "    \n",
    "    def __init__(self, cell_size=8, block_size=2, bins=9):\n",
    "        self.cell_size = cell_size\n",
    "        self.block_size = block_size\n",
    "        self.bins = bins\n",
    "    \n",
    "    def extract_hog(self, image):\n",
    "        \"\"\"Extract HOG features from image\"\"\"\n",
    "        if len(image.shape) == 3:\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        image = image.astype(float)\n",
    "        \n",
    "        # Calculate gradients\n",
    "        gx = cv2.Sobel(image, cv2.CV_64F, 1, 0, ksize=1)\n",
    "        gy = cv2.Sobel(image, cv2.CV_64F, 0, 1, ksize=1)\n",
    "        \n",
    "        magnitude = np.sqrt(gx**2 + gy**2)\n",
    "        orientation = np.arctan2(gy, gx) * 180 / np.pi\n",
    "        orientation[orientation < 0] += 180\n",
    "        \n",
    "        # Divide into cells\n",
    "        h, w = image.shape\n",
    "        n_cells_x = w // self.cell_size\n",
    "        n_cells_y = h // self.cell_size\n",
    "        \n",
    "        cell_histograms = np.zeros((n_cells_y, n_cells_x, self.bins))\n",
    "        \n",
    "        for i in range(n_cells_y):\n",
    "            for j in range(n_cells_x):\n",
    "                y_start = i * self.cell_size\n",
    "                y_end = (i + 1) * self.cell_size\n",
    "                x_start = j * self.cell_size\n",
    "                x_end = (j + 1) * self.cell_size\n",
    "                \n",
    "                cell_mag = magnitude[y_start:y_end, x_start:x_end]\n",
    "                cell_ori = orientation[y_start:y_end, x_start:x_end]\n",
    "                \n",
    "                hist, _ = np.histogram(cell_ori, bins=self.bins, range=(0, 180), weights=cell_mag)\n",
    "                cell_histograms[i, j] = hist\n",
    "        \n",
    "        # Block normalization\n",
    "        n_blocks_x = n_cells_x - self.block_size + 1\n",
    "        n_blocks_y = n_cells_y - self.block_size + 1\n",
    "        \n",
    "        normalized_blocks = []\n",
    "        \n",
    "        for i in range(n_blocks_y):\n",
    "            for j in range(n_blocks_x):\n",
    "                block = cell_histograms[i:i+self.block_size, j:j+self.block_size]\n",
    "                block_vector = block.flatten()\n",
    "                \n",
    "                # L2 normalization\n",
    "                norm = np.linalg.norm(block_vector)\n",
    "                if norm > 0:\n",
    "                    block_vector = block_vector / norm\n",
    "                \n",
    "                normalized_blocks.append(block_vector)\n",
    "        \n",
    "        return np.concatenate(normalized_blocks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac752a84",
   "metadata": {},
   "source": [
    "## HOG Feature\n",
    "\n",
    "Fungsi dari HOG pada proses ini berfungsi untuk menangkap **Bentuk Global dan Struktur Tepi dari Objek Wajah**.\n",
    "\n",
    "### Langkah 1: Hitung gradien horizontal dan vertikal\n",
    "\n",
    "```\n",
    "Gx = I(x+1, y) - I(x-1, y)\n",
    "Gy = I(x, y+1) - I(x, y-1)\n",
    "```\n",
    "\n",
    "### Langkah 2: Hitung magnitude dan orientasi\n",
    "\n",
    "```\n",
    "Magnitude (M) = sqrt(Gx² + Gy²)\n",
    "Orientasi (θ) = arctan2(Gy, Gx)\n",
    "```\n",
    "\n",
    "### Langkah 3: Histogram orientasi dalam sel kecil\n",
    "- `θ` sebagai bin histogram.\n",
    "- `M` sebagai bobot.\n",
    "\n",
    "### Langkah 4: Normalisasi blok\n",
    "\n",
    "```\n",
    "v' = v / sqrt(||v||² + ε²)\n",
    "```\n",
    "\n",
    "- `v` = vektor gabungan histogram dari blok (misal: 2×2 sel)\n",
    "- `ε` = konstanta kecil untuk menghindari pembagian dengan nol\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a4b1c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LandmarkExtractor:\n",
    "    \"\"\"Facial landmark feature extractor\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Simplified landmark points (you would use dlib/mediapipe in real implementation)\n",
    "        self.landmark_indices = [\n",
    "            # Eye corners, nose tip, mouth corners, eyebrow points\n",
    "            (0.3, 0.4), (0.7, 0.4),  # Eye corners\n",
    "            (0.5, 0.6),               # Nose tip\n",
    "            (0.35, 0.75), (0.65, 0.75), # Mouth corners\n",
    "            (0.25, 0.3), (0.75, 0.3),   # Eyebrow corners\n",
    "        ]\n",
    "    \n",
    "    def extract_landmarks(self, image):\n",
    "        \"\"\"Extract geometric features from facial landmarks\"\"\"\n",
    "        h, w = image.shape[:2]\n",
    "        \n",
    "        # Simulate landmark detection (in real scenario, use proper detector)\n",
    "        landmarks = []\n",
    "        for x_ratio, y_ratio in self.landmark_indices:\n",
    "            x = int(x_ratio * w)\n",
    "            y = int(y_ratio * h)\n",
    "            landmarks.append([x, y])\n",
    "        \n",
    "        landmarks = np.array(landmarks)\n",
    "        \n",
    "        # Calculate geometric features\n",
    "        features = []\n",
    "        \n",
    "        # Eye distance\n",
    "        eye_dist = np.linalg.norm(landmarks[1] - landmarks[0])\n",
    "        features.append(eye_dist / w)  # Normalized by width\n",
    "        \n",
    "        # Mouth width\n",
    "        mouth_width = np.linalg.norm(landmarks[4] - landmarks[3])\n",
    "        features.append(mouth_width / w)\n",
    "        \n",
    "        # Eye-mouth distance\n",
    "        eye_center = (landmarks[0] + landmarks[1]) / 2\n",
    "        mouth_center = (landmarks[3] + landmarks[4]) / 2\n",
    "        eye_mouth_dist = np.linalg.norm(mouth_center - eye_center)\n",
    "        features.append(eye_mouth_dist / h)\n",
    "        \n",
    "        # Eyebrow angle (anger indicator)\n",
    "        eyebrow_angle = landmarks[6][1] - landmarks[5][1]  # Right eyebrow - left eyebrow\n",
    "        features.append(eyebrow_angle / h)\n",
    "        \n",
    "        # Mouth curvature (happiness indicator)\n",
    "        nose_y = landmarks[2][1]\n",
    "        mouth_y = (landmarks[3][1] + landmarks[4][1]) / 2\n",
    "        mouth_curve = (nose_y - mouth_y) / h\n",
    "        features.append(mouth_curve)\n",
    "        \n",
    "        return np.array(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6e82b0",
   "metadata": {},
   "source": [
    "## Landmark/Facial Keypoints\n",
    "\n",
    "Fungsi dari landmark pada proses ini berfungsi untuk menangkap **posisi kunci dari bagian wajah** seperti mata, hidung, mulut, alis, dll.\n",
    "\n",
    "Tapi untuk proses ini, hanya mengambil posisi wajah seperti mata, hidung, mulut, dan alis.\n",
    "\n",
    "### Mapping Wajah\n",
    "\n",
    "Format dari mapping landmark menggunakan format (x, y), x dan y adalah nilai antara 0 dan 1, yang menunjukan landmark wajah.\n",
    "\n",
    "```python\n",
    "self.landmark_indices = [\n",
    "    (0.3, 0.4), (0.7, 0.4),  # Eye corners\n",
    "    (0.5, 0.6),               # Nose tip\n",
    "    (0.35, 0.75), (0.65, 0.75), # Mouth corners\n",
    "    (0.25, 0.3), (0.75, 0.3),   # Eyebrow corners\n",
    "]\n",
    "```\n",
    "\n",
    "### 1. Eye Distance (Jarak antara kedua mata)\n",
    "\n",
    "```\n",
    "eye_dist = distance(landmark_1, landmark_0) / width\n",
    "```\n",
    "\n",
    "### 2. Mouth Width (Lebar mulut)\n",
    "\n",
    "```\n",
    "mouth_width = distance(landmark_4, landmark_3) / width\n",
    "```\n",
    "\n",
    "### 3. Eye-Mouth Distance (Jarak pusat mata ke pusat mulut)\n",
    "\n",
    "```\n",
    "eye_center = (landmark_0 + landmark_1) / 2\n",
    "mouth_center = (landmark_3 + landmark_4) / 2\n",
    "eye_mouth_dist = distance(mouth_center, eye_center) / height\n",
    "```\n",
    "\n",
    "### 4. Eyebrow Angle (Arah Alis)\n",
    "\n",
    "```\n",
    "eyebrow_angle = (landmark_6_y - landmark_5_y) / height\n",
    "```\n",
    "\n",
    "### 5. Mouth Curvature (Kurva Mulut)\n",
    "\n",
    "```\n",
    "mouth_y = (landmark_3_y + landmark_4_y) / 2\n",
    "mouth_curve = (landmark_2_y - mouth_y) / height\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95336e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVM:\n",
    "    \"\"\"Support Vector Machine implementation from scratch\"\"\"\n",
    "    \n",
    "    def __init__(self, C=1.0, kernel='linear', max_iter=1000):\n",
    "        self.C = C\n",
    "        self.kernel = kernel\n",
    "        self.max_iter = max_iter\n",
    "        self.alpha = None\n",
    "        self.b = 0 # Bias\n",
    "        self.X_train = None\n",
    "        self.y_train = None\n",
    "        self.support_vectors = None\n",
    "        self.support_vector_labels = None\n",
    "        self.support_vector_alphas = None\n",
    "    \n",
    "    def kernel_function(self, x1, x2):\n",
    "        \"\"\"Compute kernel function\"\"\"\n",
    "        if self.kernel == 'linear':\n",
    "            return np.dot(x1, x2)\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Train SVM using simplified SMO algorithm\"\"\"\n",
    "        n_samples, n_features = X.shape\n",
    "        self.X_train = X\n",
    "        self.y_train = y\n",
    "        \n",
    "        # Initialize alphas\n",
    "        self.alpha = np.zeros(n_samples)\n",
    "        self.b = 0\n",
    "        \n",
    "        # Compute kernel matrix\n",
    "        K = np.zeros((n_samples, n_samples))\n",
    "        for i in range(n_samples):\n",
    "            for j in range(n_samples):\n",
    "                K[i, j] = self.kernel_function(X[i], X[j])\n",
    "        \n",
    "        # Simplified SMO algorithm\n",
    "        for iteration in range(self.max_iter):\n",
    "            alpha_prev = np.copy(self.alpha)\n",
    "            \n",
    "            for i in range(n_samples):\n",
    "                # Calculate error\n",
    "                E_i = self.decision_function_single(X[i]) - y[i]\n",
    "                \n",
    "                # Check KKT conditions\n",
    "                if (y[i] * E_i < -0.001 and self.alpha[i] < self.C) or \\\n",
    "                   (y[i] * E_i > 0.001 and self.alpha[i] > 0):\n",
    "                    \n",
    "                    # Select second alpha randomly\n",
    "                    j = np.random.choice([idx for idx in range(n_samples) if idx != i])\n",
    "                    E_j = self.decision_function_single(X[j]) - y[j]\n",
    "                    \n",
    "                    # Save old alphas\n",
    "                    alpha_i_old = self.alpha[i]\n",
    "                    alpha_j_old = self.alpha[j]\n",
    "                    \n",
    "                    # Compute bounds\n",
    "                    if y[i] != y[j]:\n",
    "                        L = max(0, self.alpha[j] - self.alpha[i])\n",
    "                        H = min(self.C, self.C + self.alpha[j] - self.alpha[i])\n",
    "                    else:\n",
    "                        L = max(0, self.alpha[i] + self.alpha[j] - self.C)\n",
    "                        H = min(self.C, self.alpha[i] + self.alpha[j])\n",
    "                    \n",
    "                    if L == H:\n",
    "                        continue\n",
    "                    \n",
    "                    # Compute eta\n",
    "                    eta = 2 * K[i, j] - K[i, i] - K[j, j]\n",
    "                    if eta >= 0:\n",
    "                        continue\n",
    "                    \n",
    "                    # Update alpha_j\n",
    "                    self.alpha[j] -= (y[j] * (E_i - E_j)) / eta\n",
    "                    self.alpha[j] = max(L, min(H, self.alpha[j]))\n",
    "                    \n",
    "                    if abs(self.alpha[j] - alpha_j_old) < 1e-5:\n",
    "                        continue\n",
    "                    \n",
    "                    # Update alpha_i\n",
    "                    self.alpha[i] += y[i] * y[j] * (alpha_j_old - self.alpha[j])\n",
    "                    \n",
    "                    # Update bias\n",
    "                    b1 = self.b - E_i - y[i] * (self.alpha[i] - alpha_i_old) * K[i, i] - \\\n",
    "                         y[j] * (self.alpha[j] - alpha_j_old) * K[i, j]\n",
    "                    b2 = self.b - E_j - y[i] * (self.alpha[i] - alpha_i_old) * K[i, j] - \\\n",
    "                         y[j] * (self.alpha[j] - alpha_j_old) * K[j, j]\n",
    "                    \n",
    "                    if 0 < self.alpha[i] < self.C:\n",
    "                        self.b = b1\n",
    "                    elif 0 < self.alpha[j] < self.C:\n",
    "                        self.b = b2\n",
    "                    else:\n",
    "                        self.b = (b1 + b2) / 2\n",
    "            \n",
    "            # Check for convergence\n",
    "            if np.allclose(self.alpha, alpha_prev, atol=1e-3):\n",
    "                break\n",
    "        \n",
    "        # Extract support vectors\n",
    "        sv_indices = self.alpha > 1e-5\n",
    "        self.support_vectors = X[sv_indices]\n",
    "        self.support_vector_labels = y[sv_indices]\n",
    "        self.support_vector_alphas = self.alpha[sv_indices]\n",
    "        \n",
    "        print(f\"Training completed. Found {len(self.support_vectors)} support vectors.\")\n",
    "    \n",
    "    def decision_function_single(self, x):\n",
    "        \"\"\"Decision function for single sample\"\"\"\n",
    "        if self.alpha is None:\n",
    "            return 0\n",
    "        \n",
    "        result = 0\n",
    "        for i in range(len(self.X_train)):\n",
    "            if self.alpha[i] > 0:\n",
    "                result += self.alpha[i] * self.y_train[i] * self.kernel_function(self.X_train[i], x)\n",
    "        \n",
    "        return result + self.b\n",
    "    \n",
    "    def decision_function(self, X):\n",
    "        \"\"\"Decision function for multiple samples\"\"\"\n",
    "        return np.array([self.decision_function_single(x) for x in X])\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict class labels\"\"\"\n",
    "        return np.sign(self.decision_function(X))\n",
    "    \n",
    "    def get_weights(self):  \n",
    "        \"\"\"Get explicit weights (only works for linear kernel)\"\"\"  \n",
    "        if self.kernel != 'linear':  \n",
    "            raise ValueError(\"Bobot cuma bisa diambil untuk kernel linear!\")  \n",
    "        \n",
    "        # w = Σ (alpha_i * y_i * x_i)  \n",
    "        w = np.zeros(self.X_train.shape[1])  # Inisialisasi vektor nol  \n",
    "        for i in range(len(self.alpha)):  \n",
    "            w += self.alpha[i] * self.y_train[i] * self.X_train[i]  \n",
    "        return w"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa75074",
   "metadata": {},
   "source": [
    "# SVM (Support Vector Machine)\n",
    "\n",
    "## 1. Kernel Function\n",
    "\n",
    "Digunakan untuk menghitung kemiripan antara dua vektor fitur `x1` dan `x2`.\n",
    "\n",
    "### Linear Kernel:\n",
    "    K(x1, x2) = x1 . x2\n",
    "\n",
    "\n",
    "## 2. Error per Sampel\n",
    "\n",
    "Menghitung error prediksi terhadap label sebenarnya:\n",
    "\n",
    "    E_i = f(x_i) - y_i\n",
    "\n",
    "Di mana `f(x_i)` adalah hasil dari.\n",
    "\n",
    "```\n",
    "    f(x_i) = Σ alpha_i * y_i * K(x_i, x_j)\n",
    "```\n",
    "\n",
    "## 3. Update Alpha j\n",
    "\n",
    "Salah satu langkah dalam algoritma SMO:\n",
    "\n",
    "    alpha_j = alpha_j - (y_j * (E_i - E_j)) / eta\n",
    "\n",
    "## 4. Second Derivative Error/Eta (η)\n",
    "\n",
    "Digunakan dalam update alpha:\n",
    "\n",
    "    eta = 2 * K(i, j) - K(i, i) - K(j, j)\n",
    "\n",
    "## 5. Update Alpha i\n",
    "\n",
    "Setelah alpha_j diupdate:\n",
    "\n",
    "    alpha_i = alpha_i + y_i * y_j * (alpha_j_old - alpha_j)\n",
    "\n",
    "## 6. Update Bias (b)\n",
    "\n",
    "Bias diperbarui menggunakan dua cara, lalu dipilih salah satu (atau rata-rata):\n",
    "\n",
    "    b1 = b - E_i - y_i * (alpha_i - alpha_i_old) * K(i, i) - y_j * (alpha_j - alpha_j_old) * K(i, j)\n",
    "    b2 = b - E_j - y_i * (alpha_i - alpha_i_old) * K(i, j) - y_j * (alpha_j - alpha_j_old) * K(j, j)\n",
    "\n",
    "Kemudian:\n",
    "\n",
    "    b = b1 (jika 0 < alpha_i < C)\n",
    "    b = b2 (jika 0 < alpha_j < C)\n",
    "    b = (b1 + b2) / 2 (jika tidak keduanya)\n",
    "\n",
    "## 7. Decision Function\n",
    "\n",
    "Digunakan untuk memprediksi skor (jarak ke hyperplane):\n",
    "\n",
    "    f(x) = Σ [alpha_i * y_i * K(x_i, x)] + b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ccad63c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmotionClassifier:\n",
    "    \"\"\"Complete emotion classification system\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.lbp = LBPExtractor()\n",
    "        self.hog = HOGExtractor()\n",
    "        self.landmark = LandmarkExtractor()\n",
    "        self.svm = SVM(C=1.0, kernel='linear')\n",
    "        self.scaler_mean = None\n",
    "        self.scaler_std = None\n",
    "    \n",
    "    def extract_features(self, image):\n",
    "        \"\"\"Extract combined LBP + HOG + Landmark features\"\"\"\n",
    "        # LBP features\n",
    "        lbp_features = self.lbp.extract_lbp(image)\n",
    "        \n",
    "        # HOG features\n",
    "        hog_features = self.hog.extract_hog(image)\n",
    "        \n",
    "        # Landmark features\n",
    "        landmark_features = self.landmark.extract_landmarks(image)\n",
    "        \n",
    "        # Combine all features\n",
    "        combined_features = np.concatenate([lbp_features, hog_features, landmark_features])\n",
    "        return combined_features\n",
    "    \n",
    "    def normalize_features(self, X, fit=False):\n",
    "        \"\"\"Normalize features using z-score normalization\"\"\"\n",
    "        if fit:\n",
    "            self.scaler_mean = np.mean(X, axis=0)\n",
    "            self.scaler_std = np.std(X, axis=0) + 1e-8\n",
    "        \n",
    "        return (X - self.scaler_mean) / self.scaler_std\n",
    "    \n",
    "    def train(self, images, labels):\n",
    "        \"\"\"Train the emotion classifier\"\"\"\n",
    "        print(\"Extracting features from training images...\")\n",
    "        \n",
    "        features = []\n",
    "        for i, image in enumerate(images):\n",
    "            print(f\"Processing image {i+1}/{len(images)}\")\n",
    "            feature_vector = self.extract_features(image)\n",
    "            features.append(feature_vector)\n",
    "        \n",
    "        X = np.array(features)\n",
    "        y = np.array(labels)  # 1 for happy, -1 for angry\n",
    "        \n",
    "        # Normalize features\n",
    "        X_normalized = self.normalize_features(X, fit=True)\n",
    "        \n",
    "        print(\"Training SVM...\")\n",
    "        self.svm.fit(X_normalized, y)\n",
    "        \n",
    "        weights = self.svm.get_weights()\n",
    "        print(\"Weight per features : \", weights)\n",
    "        \n",
    "        return X_normalized, y\n",
    "    \n",
    "    def predict(self, images):\n",
    "        \"\"\"Predict emotions for new images\"\"\"\n",
    "        features = []\n",
    "        for image in images:\n",
    "            feature_vector = self.extract_features(image)\n",
    "            features.append(feature_vector)\n",
    "        \n",
    "        X = np.array(features)\n",
    "        X_normalized = self.normalize_features(X)\n",
    "        \n",
    "        predictions = self.svm.predict(X_normalized)\n",
    "        decision_values = self.svm.decision_function(X_normalized)\n",
    "        \n",
    "        return predictions, decision_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92748a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Function for Processing data\n",
    "\n",
    "import os\n",
    "import glob\n",
    "\n",
    "def load_dataset(dataset_path):\n",
    "    \"\"\"Load images from dataset folder structure\"\"\"\n",
    "    print(f\"Loading dataset from: {dataset_path}\")\n",
    "    \n",
    "    # Paths to emotion folders\n",
    "    angry_path = os.path.join(dataset_path, \"marah\")\n",
    "    happy_path = os.path.join(dataset_path, \"senang\")\n",
    "    \n",
    "    # Check if folders exist\n",
    "    if not os.path.exists(angry_path):\n",
    "        raise FileNotFoundError(f\"Folder 'marah' not found in {dataset_path}\")\n",
    "    if not os.path.exists(happy_path):\n",
    "        raise FileNotFoundError(f\"Folder 'senang' not found in {dataset_path}\")\n",
    "    \n",
    "    # Load angry images\n",
    "    angry_images = []\n",
    "    angry_files = glob.glob(os.path.join(angry_path, \"*\"))\n",
    "    angry_files = [f for f in angry_files if f.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.tiff'))]\n",
    "    \n",
    "    print(f\"Loading {len(angry_files)} angry images...\")\n",
    "    for i, img_path in enumerate(angry_files):\n",
    "        try:\n",
    "            img = cv2.imread(img_path)\n",
    "            if img is not None:\n",
    "                # Resize to standard size for consistent feature extraction\n",
    "                img = cv2.resize(img, (128, 128))\n",
    "                angry_images.append(img)\n",
    "                if (i + 1) % 20 == 0:\n",
    "                    print(f\"  Loaded {i + 1}/{len(angry_files)} angry images\")\n",
    "            else:\n",
    "                print(f\"  Warning: Could not load {img_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  Error loading {img_path}: {e}\")\n",
    "    \n",
    "    # Load happy images\n",
    "    happy_images = []\n",
    "    happy_files = glob.glob(os.path.join(happy_path, \"*\"))\n",
    "    happy_files = [f for f in happy_files if f.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.tiff'))]\n",
    "    \n",
    "    print(f\"Loading {len(happy_files)} happy images...\")\n",
    "    for i, img_path in enumerate(happy_files):\n",
    "        try:\n",
    "            img = cv2.imread(img_path)\n",
    "            if img is not None:\n",
    "                # Resize to standard size for consistent feature extraction\n",
    "                img = cv2.resize(img, (128, 128))\n",
    "                happy_images.append(img)\n",
    "                if (i + 1) % 20 == 0:\n",
    "                    print(f\"  Loaded {i + 1}/{len(happy_files)} happy images\")\n",
    "            else:\n",
    "                print(f\"  Warning: Could not load {img_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  Error loading {img_path}: {e}\")\n",
    "    \n",
    "    print(f\"\\nDataset loaded successfully:\")\n",
    "    print(f\"- Angry images: {len(angry_images)}\")\n",
    "    print(f\"- Happy images: {len(happy_images)}\")\n",
    "    \n",
    "    return angry_images, happy_images\n",
    "\n",
    "def split_dataset(images, labels, test_ratio=0.2, random_seed=42):\n",
    "    \"\"\"Split dataset into train and test sets\"\"\"\n",
    "    np.random.seed(random_seed)\n",
    "    \n",
    "    n_samples = len(images)\n",
    "    indices = np.arange(n_samples)\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    n_test = int(n_samples * test_ratio)\n",
    "    n_train = n_samples - n_test\n",
    "    \n",
    "    train_indices = indices[:n_train]\n",
    "    test_indices = indices[n_train:]\n",
    "    \n",
    "    train_images = [images[i] for i in train_indices]\n",
    "    train_labels = [labels[i] for i in train_indices]\n",
    "    test_images = [images[i] for i in test_indices]\n",
    "    test_labels = [labels[i] for i in test_indices]\n",
    "    \n",
    "    return train_images, train_labels, test_images, test_labels\n",
    "\n",
    "def visualize_hyperplane_2d(classifier, X, y, feature_indices=[0, 1]):\n",
    "    \"\"\"Visualize SVM hyperplane in 2D using selected features\"\"\"\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Select two features for visualization\n",
    "    X_2d = X[:, feature_indices]\n",
    "    \n",
    "    # Create a mesh for plotting decision boundary\n",
    "    h = 0.1\n",
    "    x_min, x_max = X_2d[:, 0].min() - 1, X_2d[:, 0].max() + 1\n",
    "    y_min, y_max = X_2d[:, 1].min() - 1, X_2d[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    \n",
    "    # Create temporary SVM for 2D visualization\n",
    "    svm_2d = SVM(C=classifier.svm.C, kernel=classifier.svm.kernel, gamma=classifier.svm.gamma)\n",
    "    svm_2d.fit(X_2d, y)\n",
    "    \n",
    "    # Predict on mesh\n",
    "    mesh_points = np.c_[xx.ravel(), yy.ravel()]\n",
    "    Z = svm_2d.decision_function(mesh_points)\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    print(f\"Z Min : {Z.min()}, Z Max : {Z.max()}\")\n",
    "    \n",
    "    print(\"\\nClassifier Info\")\n",
    "    print(\"===============================\")\n",
    "    print(\"Regularization parameter Value : \", classifier.svm.C)\n",
    "    print(\"Kernel : \", classifier.svm.kernel)\n",
    "    \n",
    "    # Plot decision boundary and margins\n",
    "    plt.contour(xx, yy, Z, levels=[-1, 0, 1], alpha=0.5, \n",
    "                linestyles=['--', '-', '--'], colors=['red', 'black', 'red'])\n",
    "    plt.contourf(xx, yy, Z, levels=50, alpha=0.3, cmap='RdYlBu')\n",
    "    \n",
    "    # Plot data points\n",
    "    happy_mask = y == 1\n",
    "    angry_mask = y == -1\n",
    "    \n",
    "    plt.scatter(X_2d[happy_mask, 0], X_2d[happy_mask, 1], \n",
    "                c='blue', marker='o', s=50, label='Happy', alpha=0.7)\n",
    "    plt.scatter(X_2d[angry_mask, 0], X_2d[angry_mask, 1], \n",
    "                c='red', marker='s', s=50, label='Angry', alpha=0.7)\n",
    "    \n",
    "    # Highlight support vectors\n",
    "    if svm_2d.support_vectors is not None:\n",
    "        plt.scatter(svm_2d.support_vectors[:, 0], svm_2d.support_vectors[:, 1],\n",
    "                    s=100, facecolors='none', edgecolors='black', linewidth=2,\n",
    "                    label='Support Vectors')\n",
    "    \n",
    "    plt.xlabel(f'Feature {feature_indices[0]} (Normalized)')\n",
    "    plt.ylabel(f'Feature {feature_indices[1]} (Normalized)')\n",
    "    plt.title('SVM Hyperplane Visualization\\n(LBP + HOG + Landmark Features)')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.colorbar(label='Decision Function Value')\n",
    "    plt.show()\n",
    "\n",
    "def evaluate_model(classifier, test_images, test_labels):\n",
    "    \"\"\"Evaluate model performance\"\"\"\n",
    "    print(\"\\nEvaluating model on test set...\")\n",
    "    \n",
    "    # Make predictions\n",
    "    test_predictions, test_decision_values = classifier.predict(test_images)\n",
    "    \n",
    "    test_predictions = test_predictions.astype(int)\n",
    "    \n",
    "    test_labels = np.array(test_labels)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = np.mean(test_predictions == test_labels)\n",
    "    \n",
    "    # Confusion matrix\n",
    "    tp = np.sum((test_predictions == 1) & (test_labels == 1))  # True Positive (Happy)\n",
    "    tn = np.sum((test_predictions == -1) & (test_labels == -1))  # True Negative (Angry)\n",
    "    fp = np.sum((test_predictions == 1) & (test_labels == -1))  # False Positive\n",
    "    fn = np.sum((test_predictions == -1) & (test_labels == 1))  # False Negative\n",
    "    \n",
    "    precision_happy = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall_happy = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    precision_angry = tn / (tn + fn) if (tn + fn) > 0 else 0\n",
    "    recall_angry = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "    \n",
    "    f1_happy = 2 * (precision_happy * recall_happy) / (precision_happy + recall_happy) if (precision_happy + recall_happy) > 0 else 0\n",
    "    f1_angry = 2 * (precision_angry * recall_angry) / (precision_angry + recall_angry) if (precision_angry + recall_angry) > 0 else 0\n",
    "    \n",
    "    print(f\"\\n=== Test Results ===\")\n",
    "    print(f\"Test Accuracy: {accuracy:.2%}\")\n",
    "    print(f\"\\nConfusion Matrix:\")\n",
    "    print(f\"                Predicted\")\n",
    "    print(f\"              Happy  Angry\")\n",
    "    print(f\"Actual Happy    {tp:3d}    {fn:3d}\")\n",
    "    print(f\"       Angry    {fp:3d}    {tn:3d}\")\n",
    "    \n",
    "    print(f\"\\nPer-class Performance:\")\n",
    "    print(f\"Happy  - Precision: {precision_happy:.3f}, Recall: {recall_happy:.3f}, F1: {f1_happy:.3f}\")\n",
    "    print(f\"Angry  - Precision: {precision_angry:.3f}, Recall: {recall_angry:.3f}, F1: {f1_angry:.3f}\")\n",
    "    \n",
    "    return test_predictions, test_decision_values, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9d420956",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== SVM Emotion Classification ===\n",
      "Features: LBP + HOG + Landmark\n",
      "Dataset: marah + senang\n",
      "Loading dataset from: data_roi\n",
      "Loading 67 angry images...\n",
      "  Loaded 20/67 angry images\n",
      "  Loaded 40/67 angry images\n",
      "  Loaded 60/67 angry images\n",
      "Loading 65 happy images...\n",
      "  Loaded 20/65 happy images\n",
      "  Loaded 40/65 happy images\n",
      "  Loaded 60/65 happy images\n",
      "\n",
      "Dataset loaded successfully:\n",
      "- Angry images: 67\n",
      "- Happy images: 65\n"
     ]
    }
   ],
   "source": [
    "# Main function for emotion classification with real dataset\n",
    "print(\"=== SVM Emotion Classification ===\")\n",
    "print(\"Features: LBP + HOG + Landmark\")\n",
    "print(\"Dataset: marah + senang\")\n",
    "\n",
    "# *** CHANGE THIS PATH TO YOUR DATASET FOLDER ***\n",
    "dataset_path = \"data_roi\"  # Change this to your actual dataset path\n",
    "\n",
    "# Load dataset\n",
    "angry_images, happy_images = load_dataset(dataset_path)\n",
    "\n",
    "if len(angry_images) == 0 or len(happy_images) == 0:\n",
    "    raise ValueError(\"No images loaded. Check dataset path and image formats.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7b83e347",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset Split:\n",
      "- Training: 106 images\n",
      "- Testing:  26 images\n"
     ]
    }
   ],
   "source": [
    "# Prepare data\n",
    "all_images = angry_images + happy_images\n",
    "all_labels = [-1] * len(angry_images) + [1] * len(happy_images)  # -1=angry, 1=happy\n",
    "\n",
    "# Split dataset\n",
    "train_images, train_labels, test_images, test_labels = split_dataset(\n",
    "    all_images, all_labels, test_ratio=0.2, random_seed=42\n",
    ")\n",
    "\n",
    "print(f\"\\nDataset Split:\")\n",
    "print(f\"- Training: {len(train_images)} images\")\n",
    "print(f\"- Testing:  {len(test_images)} images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "de6283c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training emotion classifier...\n",
      "Extracting features from training images...\n",
      "Processing image 1/106\n",
      "Processing image 2/106\n",
      "Processing image 3/106\n",
      "Processing image 4/106\n",
      "Processing image 5/106\n",
      "Processing image 6/106\n",
      "Processing image 7/106\n",
      "Processing image 8/106\n",
      "Processing image 9/106\n",
      "Processing image 10/106\n",
      "Processing image 11/106\n",
      "Processing image 12/106\n",
      "Processing image 13/106\n",
      "Processing image 14/106\n",
      "Processing image 15/106\n",
      "Processing image 16/106\n",
      "Processing image 17/106\n",
      "Processing image 18/106\n",
      "Processing image 19/106\n",
      "Processing image 20/106\n",
      "Processing image 21/106\n",
      "Processing image 22/106\n",
      "Processing image 23/106\n",
      "Processing image 24/106\n",
      "Processing image 25/106\n",
      "Processing image 26/106\n",
      "Processing image 27/106\n",
      "Processing image 28/106\n",
      "Processing image 29/106\n",
      "Processing image 30/106\n",
      "Processing image 31/106\n",
      "Processing image 32/106\n",
      "Processing image 33/106\n",
      "Processing image 34/106\n",
      "Processing image 35/106\n",
      "Processing image 36/106\n",
      "Processing image 37/106\n",
      "Processing image 38/106\n",
      "Processing image 39/106\n",
      "Processing image 40/106\n",
      "Processing image 41/106\n",
      "Processing image 42/106\n",
      "Processing image 43/106\n",
      "Processing image 44/106\n",
      "Processing image 45/106\n",
      "Processing image 46/106\n",
      "Processing image 47/106\n",
      "Processing image 48/106\n",
      "Processing image 49/106\n",
      "Processing image 50/106\n",
      "Processing image 51/106\n",
      "Processing image 52/106\n",
      "Processing image 53/106\n",
      "Processing image 54/106\n",
      "Processing image 55/106\n",
      "Processing image 56/106\n",
      "Processing image 57/106\n",
      "Processing image 58/106\n",
      "Processing image 59/106\n",
      "Processing image 60/106\n",
      "Processing image 61/106\n",
      "Processing image 62/106\n",
      "Processing image 63/106\n",
      "Processing image 64/106\n",
      "Processing image 65/106\n",
      "Processing image 66/106\n",
      "Processing image 67/106\n",
      "Processing image 68/106\n",
      "Processing image 69/106\n",
      "Processing image 70/106\n",
      "Processing image 71/106\n",
      "Processing image 72/106\n",
      "Processing image 73/106\n",
      "Processing image 74/106\n",
      "Processing image 75/106\n",
      "Processing image 76/106\n",
      "Processing image 77/106\n",
      "Processing image 78/106\n",
      "Processing image 79/106\n",
      "Processing image 80/106\n",
      "Processing image 81/106\n",
      "Processing image 82/106\n",
      "Processing image 83/106\n",
      "Processing image 84/106\n",
      "Processing image 85/106\n",
      "Processing image 86/106\n",
      "Processing image 87/106\n",
      "Processing image 88/106\n",
      "Processing image 89/106\n",
      "Processing image 90/106\n",
      "Processing image 91/106\n",
      "Processing image 92/106\n",
      "Processing image 93/106\n",
      "Processing image 94/106\n",
      "Processing image 95/106\n",
      "Processing image 96/106\n",
      "Processing image 97/106\n",
      "Processing image 98/106\n",
      "Processing image 99/106\n",
      "Processing image 100/106\n",
      "Processing image 101/106\n",
      "Processing image 102/106\n",
      "Processing image 103/106\n",
      "Processing image 104/106\n",
      "Processing image 105/106\n",
      "Processing image 106/106\n",
      "Training SVM...\n",
      "Training completed. Found 75 support vectors.\n",
      "Weight per features :  [-0.00154885 -0.00149514 -0.00160508 ...  0.          0.\n",
      "  0.        ]\n",
      "\n",
      "Feature Analysis:\n",
      "- Total feature vector size: 8361\n",
      "- LBP features: 256\n",
      "- HOG features: 8100\n",
      "- Landmark features: 5\n"
     ]
    }
   ],
   "source": [
    "# Initialize classifier\n",
    "classifier = EmotionClassifier()\n",
    "\n",
    "# Train the classifier\n",
    "print(\"\\nTraining emotion classifier...\")\n",
    "X_train, y_train = classifier.train(train_images, train_labels)\n",
    "\n",
    "# Display feature information\n",
    "print(f\"\\nFeature Analysis:\")\n",
    "print(f\"- Total feature vector size: {X_train.shape[1]}\")\n",
    "print(f\"- LBP features: {classifier.lbp.extract_lbp(train_images[0]).shape[0]}\")\n",
    "print(f\"- HOG features: {classifier.hog.extract_hog(train_images[0]).shape[0]}\")\n",
    "print(f\"- Landmark features: {classifier.landmark.extract_landmarks(train_images[0]).shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ffe4fc27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(106, 8361)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "14a8a334",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"Feature Matrix.csv\", X_train, delimiter=\",\")\n",
    "np.savetxt(\"Target Vector.csv\", y_train, delimiter=\",\", fmt=\"%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "74f983e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Accuracy: 88.68%\n"
     ]
    }
   ],
   "source": [
    "# Training accuracy\n",
    "train_predictions, train_decision_values = classifier.predict(train_images)\n",
    "train_accuracy = np.mean(train_predictions == y_train)\n",
    "print(f\"\\nTraining Accuracy: {train_accuracy:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a2d5737e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating model on test set...\n",
      "\n",
      "=== Test Results ===\n",
      "Test Accuracy: 57.69%\n",
      "\n",
      "Confusion Matrix:\n",
      "                Predicted\n",
      "              Happy  Angry\n",
      "Actual Happy      9      7\n",
      "       Angry      4      6\n",
      "\n",
      "Per-class Performance:\n",
      "Happy  - Precision: 0.692, Recall: 0.562, F1: 0.621\n",
      "Angry  - Precision: 0.462, Recall: 0.600, F1: 0.522\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "test_predictions, test_decision_values, test_accuracy = evaluate_model(\n",
    "    classifier, test_images, test_labels\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6d9e5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Visualizing SVM hyperplane...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Lang\\python\\Lib\\site-packages\\numpy\\lib\\_function_base_impl.py:2999: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[:, None]\n",
      "c:\\Lang\\python\\Lib\\site-packages\\numpy\\lib\\_function_base_impl.py:3000: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[None, :]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'SVM' object has no attribute 'gamma'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 28\u001b[39m\n\u001b[32m     18\u001b[39m top_features = np.argsort(feature_importance)[-\u001b[32m2\u001b[39m:]\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# top_features = np.arange(X_train.shape[1])[valid_indices][top_valid_indices]\u001b[39;00m\n\u001b[32m     20\u001b[39m \n\u001b[32m     21\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m     25\u001b[39m \u001b[38;5;66;03m# print(\"\\nTop Features : \")\u001b[39;00m\n\u001b[32m     26\u001b[39m \u001b[38;5;66;03m# print(top_features)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m \u001b[43mvisualize_hyperplane_2d\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclassifier\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_features\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[38;5;66;03m# Display sample images from dataset\u001b[39;00m\n\u001b[32m     31\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mDisplaying sample images...\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 102\u001b[39m, in \u001b[36mvisualize_hyperplane_2d\u001b[39m\u001b[34m(classifier, X, y, feature_indices)\u001b[39m\n\u001b[32m     98\u001b[39m xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n\u001b[32m     99\u001b[39m                      np.arange(y_min, y_max, h))\n\u001b[32m    101\u001b[39m \u001b[38;5;66;03m# Create temporary SVM for 2D visualization\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m102\u001b[39m svm_2d = SVM(C=classifier.svm.C, kernel=classifier.svm.kernel, gamma=\u001b[43mclassifier\u001b[49m\u001b[43m.\u001b[49m\u001b[43msvm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgamma\u001b[49m)\n\u001b[32m    103\u001b[39m svm_2d.fit(X_2d, y)\n\u001b[32m    105\u001b[39m \u001b[38;5;66;03m# Predict on mesh\u001b[39;00m\n",
      "\u001b[31mAttributeError\u001b[39m: 'SVM' object has no attribute 'gamma'"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize hyperplane\n",
    "print(\"\\nVisualizing SVM hyperplane...\")\n",
    "\n",
    "# # Select most discriminative features for visualization\n",
    "# #feature_importance = np.abs(np.corrcoef(X_train.T, y_train)[:-1, -1])\n",
    "# feature_importance = np.abs(np.corrcoef(X_train.T, y_train)[:-1, -1])\n",
    "# top_features = np.argsort(feature_importance)[-2:]\n",
    "\n",
    "# Hitung korelasi\n",
    "correlation = np.corrcoef(X_train.T, y_train)[:-1, -1]\n",
    "\n",
    "# Buang NaN\n",
    "valid_indices = ~np.isnan(correlation)\n",
    "feature_importance = np.abs(correlation[valid_indices])\n",
    "\n",
    "# Ambil top 2 dari yang valid\n",
    "# top_valid_indices = np.argsort(feature_importance)[-2:]\n",
    "top_features = np.argsort(feature_importance)[-2:]\n",
    "# top_features = np.arange(X_train.shape[1])[valid_indices][top_valid_indices]\n",
    "\n",
    "\n",
    "# print(\"\\nFeature Importance : \")\n",
    "# print(feature_importance)\n",
    "\n",
    "# print(\"\\nTop Features : \")\n",
    "# print(top_features)\n",
    "\n",
    "visualize_hyperplane_2d(classifier, X_train, y_train, top_features)\n",
    "\n",
    "# Display sample images from dataset\n",
    "print(\"\\nDisplaying sample images...\")\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Show happy samples\n",
    "plt.subplot(3, 6, 1)\n",
    "plt.text(0.5, 0.5, \"HAPPY\\nSAMPLES\", ha='center', va='center', \n",
    "        fontsize=12, fontweight='bold', transform=plt.gca().transAxes)\n",
    "plt.axis('off')\n",
    "\n",
    "for i in range(5):\n",
    "    if i < len(happy_images):\n",
    "        plt.subplot(3, 6, i+2)\n",
    "        img_display = cv2.cvtColor(happy_images[i], cv2.COLOR_BGR2RGB)\n",
    "        plt.imshow(img_display)\n",
    "        plt.title(f'Happy {i+1}', fontsize=10)\n",
    "        plt.axis('off')\n",
    "\n",
    "# Show angry samples  \n",
    "plt.subplot(3, 6, 7)\n",
    "plt.text(0.5, 0.5, \"ANGRY\\nSAMPLES\", ha='center', va='center', \n",
    "        fontsize=12, fontweight='bold', transform=plt.gca().transAxes)\n",
    "plt.axis('off')\n",
    "\n",
    "for i in range(5):\n",
    "    if i < len(angry_images):\n",
    "        plt.subplot(3, 6, i+8)\n",
    "        img_display = cv2.cvtColor(angry_images[i], cv2.COLOR_BGR2RGB)\n",
    "        plt.imshow(img_display)\n",
    "        plt.title(f'Angry {i+1}', fontsize=10)\n",
    "        plt.axis('off')\n",
    "\n",
    "# Show some test predictions\n",
    "plt.subplot(3, 6, 13)\n",
    "plt.text(0.5, 0.5, \"TEST\\nPREDICTIONS\", ha='center', va='center', \n",
    "        fontsize=12, fontweight='bold', transform=plt.gca().transAxes)\n",
    "plt.axis('off')\n",
    "\n",
    "emotion_names = {1: 'Happy', -1: 'Angry'}\n",
    "for i in range(5):\n",
    "    if i < len(test_images):\n",
    "        plt.subplot(3, 6, i+14)\n",
    "        img_display = cv2.cvtColor(test_images[i], cv2.COLOR_BGR2RGB)\n",
    "        plt.imshow(img_display)\n",
    "        \n",
    "        true_label = emotion_names[test_labels[i]]\n",
    "        pred_label = emotion_names[int(test_predictions[i])]\n",
    "        confidence = abs(test_decision_values[i])\n",
    "        \n",
    "        color = 'green' if test_predictions[i] == test_labels[i] else 'red'\n",
    "        plt.title(f'T:{true_label}\\nP:{pred_label}\\n({confidence:.2f})', \n",
    "                fontsize=8, color=color)\n",
    "        plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c4a37f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Classification Complete ===\n",
      "Dataset: 67 angry + 65 happy images\n",
      "Training Accuracy: 88.68%\n",
      "Test Accuracy: 57.69%\n",
      "Support Vectors: 75\n",
      "SVM Parameters: C=1.0, kernel=linear\n"
     ]
    }
   ],
   "source": [
    "# Final summary\n",
    "print(f\"\\n=== Classification Complete ===\")\n",
    "print(f\"Dataset: {len(angry_images)} angry + {len(happy_images)} happy images\")\n",
    "print(f\"Training Accuracy: {train_accuracy:.2%}\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.2%}\")\n",
    "print(f\"Support Vectors: {len(classifier.svm.support_vectors)}\")\n",
    "print(f\"SVM Parameters: C={classifier.svm.C}, kernel={classifier.svm.kernel}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cfc1151",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample Test Predictions:\n",
      "✓ Test  1: True=Angry, Pred=Angry, Confidence=1.431\n",
      "✗ Test  2: True=Angry, Pred=Happy, Confidence=0.091\n",
      "✓ Test  3: True=Happy, Pred=Happy, Confidence=0.492\n",
      "✓ Test  4: True=Happy, Pred=Happy, Confidence=0.200\n",
      "✓ Test  5: True=Angry, Pred=Angry, Confidence=0.328\n",
      "✓ Test  6: True=Angry, Pred=Angry, Confidence=0.226\n",
      "✓ Test  7: True=Angry, Pred=Angry, Confidence=1.017\n",
      "✓ Test  8: True=Angry, Pred=Angry, Confidence=0.386\n",
      "✗ Test  9: True=Angry, Pred=Happy, Confidence=0.146\n",
      "✓ Test 10: True=Happy, Pred=Happy, Confidence=0.081\n"
     ]
    }
   ],
   "source": [
    "# Show some individual predictions\n",
    "print(f\"\\nSample Test Predictions:\")\n",
    "for i in range(min(10, len(test_images))):\n",
    "    true_label = emotion_names[test_labels[i]]\n",
    "    pred_label = emotion_names[int(test_predictions[i])]\n",
    "    confidence = abs(test_decision_values[i])\n",
    "    status = \"✓\" if test_predictions[i] == test_labels[i] else \"✗\"\n",
    "    print(f\"{status} Test {i+1:2d}: True={true_label:5s}, Pred={pred_label:5s}, \"\n",
    "            f\"Confidence={confidence:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
